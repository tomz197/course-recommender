{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "\n",
    "from scripts.helpers import load_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21106"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = load_courses('./data/generated/')[0]\n",
    "len(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (terms): 5000\n",
      "TF-IDF matrix shape: (21106, 5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract text data from courses for TF-IDF processing\n",
    "# We'll combine relevant text fields from each course to create a corpus\n",
    "corpus = []\n",
    "course_codes = []\n",
    "\n",
    "for course in courses:\n",
    "    # Combine relevant text fields into a single document\n",
    "    # document = f\"{course['NAME']} {course['SYLLABUS']} {course['OBJECTIVES']} {course['LEARNING_OUTCOMES']} {course['DESCRIPTION']}\"\n",
    "    document = json.dumps(course, ensure_ascii=False)\n",
    "\n",
    "    # Add keywords if available\n",
    "    if 'KEYWORDS' in course and course['KEYWORDS']:\n",
    "        document += \" \" + \" \".join(course['KEYWORDS'])\n",
    "\n",
    "    corpus.append(document)\n",
    "    course_codes.append(course['CODE'])\n",
    "\n",
    "# Initialize and fit the TF-IDF vectorizer\n",
    "# max_features limits the vocabulary size to the most important terms\n",
    "# stop_words removes common English words that don't carry much meaning\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True  # Apply sublinear tf scaling (1 + log(tf))\n",
    ")\n",
    "\n",
    "# Transform the corpus into TF-IDF features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of features (terms): {len(feature_names)}\")\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top terms for course bk4001 - Methodology:\n",
      "  renata: 0.1700\n",
      "  citation: 0.1672\n",
      "  hendl: 0.1521\n",
      "  statistical: 0.1364\n",
      "  kvalitativní: 0.1353\n"
     ]
    }
   ],
   "source": [
    "# Example: Get the top 5 terms for the first course\n",
    "def get_top_terms(doc_idx, top_n=5):\n",
    "    feature_index = tfidf_matrix[doc_idx].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc_idx, x] for x in feature_index])\n",
    "    sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "    return [(feature_names[idx], score) for idx, score in sorted_scores[:top_n]]\n",
    "\n",
    "# Display top terms for the first course\n",
    "if len(courses) > 0:\n",
    "    print(f\"\\nTop terms for course {course_codes[0]} - {courses[0]['NAME']}:\")\n",
    "    for term, score in get_top_terms(0):\n",
    "        print(f\"  {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CODE': 'IB111', 'FACULTY': 'FI', 'NAME': 'Foundations of Programming', 'LANGUAGE': 'čeština', 'SEMESTER': 'podzim 2024', 'CREDITS': '5', 'DEPARTMENT': 'KPSK', 'TEACHERS': 'Beneš, N. - Bartek, F. - Bednařík, K. - Borošová, K. - Brdečko, V. - Bukor, O. - Burget, J. - Čepela, S. - Focko, M. - Foltýnek, T. - Glosner, R. - Jedelský, J. - Juračková, N. - Kasprzaková, I. - Lukačovič, B. - Marek, T. - Melkovič, D. - Pastva, S. - Patlevič, M. - Rakšány, P. - Ročkai, P. - Ručka, L. - Sedlák, E. - Stančík, S. - Šutor, D. - Trnavský, P. - Tuček, M. - Tvarožek, M. - Uhlík, V. - Vojnar, T. - Weinberger, F. - Winklerová, A. - Wolek, J. - Záborský, L. - Zatloukal, J. - Žbánek, V. - Balák, T. - Baník, R. - Barna, M. - Béreš, J. - Biačko, P. - Borský, J. - Bukáček, M. - Čech, R. - Čermák, K. - Černá, I. - Davidová, N. - Drkoš, T. - Dvořák, R. - Ergang, M. - Fedorko, F. - Frejlach, J. - Glos, J. - Hadar, A. - Halabala, J. - Halamka, M. - Hejčl, P. - Horák, J. - Jarošová, J. - Judiny, J. - Kamenov, D. - Kapko, J. - Kecskésová, M. - Kinská, T. - Klapetek, V. - Klostermann, T. - Korž, M. - Kotúček, P. - Krchňák, T. - Kubica, P. - Kubík, A. - Lacko, R. - Lopatka, A. - Ludvig, L. - Mackovík, M. - Marcinech, M. - Martišová, S. - Matuška, J. - Metelka, O. - Nadzam, M. - Novák, P. - Ondulič, M. - Pavelka, A. - Pavlovič, F. - Piatková, B. - Pittner, L. - Rábek, M. - Rádl, J. - Rohlínek, T. - Řechtáčková, A. - Sabo, J. - Simandl, J. - Sviatková, S. - Szalona, G. - Tejbus, A. - Tomíček, T. - Vajda, P. - Valalský, A. - Valková, D. - Zemančík, J. - Zemanová, V. - Žižka, J.', 'COMPLETION': 'zk', 'PREREQUISITES': 'předp. ! IB113 && ! NOW ( IB113 )', 'FIELDS_OF_STUDY': 'DL, IN, PVA, BCS, INFMAJ', 'TYPE_OF_STUDY': 'bakalářský, magisterský navazující', 'LECTURES_SEMINARS_HOMEWORK': '2/2/2', 'SYLLABUS': 'The course shows the basic elements of imperative programming and algorithmic thinking using the high-level programming language Python as an example.\\n    Basic notions of imperative programming languages: variables and their semantics, expressions and statements, branching, cycles; subroutines (functions), passing parameters (calling functions), pure functions, predicates.\\n    Numerical computation, basic data types, using the random generator.\\n    Data structures, ADT, lists, strings, multidimensional arrays, sets, dictionaries, the basic of using objects to create user-defined data structures.\\n    The basics of testing and debugging, preconditions and postconditions, type annotation.\\n    Examples of basic algorithms: greatest common divisor, prime numbers, sorting algorithms, searching.\\n    The efficiency of algorithms, the basics of complexity, the complexity of basic data structures operations.\\n    Recursion and its specifics in the imperative paradigm, tail recursion; using recursion to work with tree data structures and to solve constraint satisfaction problems (the basics of the backtracking technique).\\n    Interaction with the environment (I/O), turtle graphics, bitmap graphics, text processing.\\n    Program design, programming styles and conventions, readability and maintainability of code, documentation and comments.', 'OBJECTIVES': 'The course is an introduction to programming and algorithmic style of thinking.', 'TEXT_PREREQUISITS': '', 'ASSESMENT_METHODS': 'Assesment consists of 3 parts: homeworks, mid-term and final programming tests, final written test.', 'TEACHING_METHODS': 'lectures, programming seminars, homeworks (programming)', 'TEACHER_INFO': '', 'LEARNING_OUTCOMES': 'At the end of the course students should be able to: understand and apply basic constructs of programming languages (e.g., conditions, loops, functions, basic data types); write and debug a program in Python; use basic data types and structures (strings, lists, dictionaries); describe several basic algorithms; describe main conventions and recommended programming style.', 'LITERATURE': '\\n        PELÁNEK, Radek. Programátorská cvičebnice: algoritmy v příkladech. Brno: Computer Press, 2012, 175 s. ISBN 978-80-251-3751-2. info\\n        GUZDIAL, Mark a Barbara ERICSON. Introduction to computing & programming in Python : a multimedia approach. 2nd ed. Upper Saddle River [N.J.]: Prentice Hall, 2010, xxiii, 401. ISBN 9780136060239. info\\n        ZELLE, John M. Python programming : an introduction to computer science. Wilsonville: Franklin, Beedle &Associates, 2004, xiv, 514. ISBN 1887902996. info \\n    ', 'STUDENTS_ENROLLED': '786', 'STUDENTS_PASSED': '332', 'AVERAGE_GRADE': '3.05', 'FOLLOWUP_COURSES': 'IB002,PB111,PV248', 'KEYWORDS': ['programming', 'python', 'imperative programming', 'algorithmic thinking', 'data structures', 'debugging', 'program design', 'algorithms', 'code', 'functions', 'loops', 'basic data types', 'testing', 'recursion', 'i/o'], 'DESCRIPTION': \"Want to learn programming from the ground up? This course introduces imperative programming and algorithmic thinking using Python. You'll learn basic data structures, debugging, and program design. Get hands-on experience with programming seminars and homework assignments, and learn to write clean, maintainable code.\", 'RATINGS': {'theoretical_vs_practical': '6', 'usefulness': '8', 'interest': '7', 'stem_vs_humanities': '2', 'abstract_vs_specific': '7', 'difficulty': '5', 'multidisciplinary': '3', 'project_based': '7', 'creative': '5'}}\n"
     ]
    }
   ],
   "source": [
    "code = \"IB111\"\n",
    "\n",
    "for course in courses:\n",
    "    if course['CODE'] == code:\n",
    "        print(course)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 terms for course PB111 - Principles of low-level programming:\n",
      "  programming: 0.2384\n",
      "  memory: 0.2221\n",
      "  allocation: 0.1924\n",
      "  computational: 0.1864\n",
      "  tables: 0.1685\n",
      "  algorithms: 0.1678\n",
      "  dynamic: 0.1662\n",
      "  machine: 0.1586\n",
      "  low: 0.1518\n",
      "  search: 0.1326\n",
      "  blocks: 0.1265\n",
      "  variable: 0.1249\n",
      "  block: 0.1229\n",
      "  linked: 0.1206\n",
      "  computer: 0.1198\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the course with code 'IB111'\n",
    "ib111_idx = None\n",
    "for i, code in enumerate(course_codes):\n",
    "    if code == 'PB111':\n",
    "        ib111_idx = i\n",
    "        break\n",
    "\n",
    "# Display top 15 terms for IB111 if found\n",
    "if ib111_idx is not None:\n",
    "    course_name = courses[ib111_idx]['NAME']\n",
    "    print(f\"\\nTop 15 terms for course {course_codes[ib111_idx]} - {course_name}:\")\n",
    "    for term, score in get_top_terms(ib111_idx, top_n=15):\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCourse with code 'IB111' not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(course_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting top keywords for each course...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21106/21106 [01:01<00:00, 345.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top keywords saved to course_top_keywords.json\n",
      "\n",
      "Sample of extracted keywords:\n",
      "\n",
      "Course bk4001:\n",
      "  renata: 0.1700\n",
      "  citation: 0.1672\n",
      "  hendl: 0.1521\n",
      "  statistical: 0.1364\n",
      "  kvalitativní: 0.1353\n",
      "  ...\n",
      "\n",
      "Course bk4003:\n",
      "  sports: 0.2192\n",
      "  spelling: 0.1830\n",
      "  anatomy: 0.1686\n",
      "  cefr: 0.1611\n",
      "  sport: 0.1565\n",
      "  ...\n",
      "\n",
      "Course bk4005:\n",
      "  pedagogy: 0.1745\n",
      "  socialization: 0.1744\n",
      "  průcha: 0.1690\n",
      "  pedagogical: 0.1625\n",
      "  youth: 0.1490\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Extract top 15 keywords for each course and store in a JSON file\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dictionary to store course codes and their top keywords\n",
    "course_keywords = {}\n",
    "\n",
    "# Process each course\n",
    "print(\"Extracting top keywords for each course...\")\n",
    "for idx, code in enumerate(tqdm(course_codes)):\n",
    "    # Get the top 15 terms for this course\n",
    "    top_terms = get_top_terms(idx, top_n=15)\n",
    "\n",
    "    # Store as a dictionary with term and score\n",
    "    item = [{\"term\": term, \"score\": float(score)} for term, score in top_terms]\n",
    "    if code not in course_keywords:\n",
    "        course_keywords[code] = item\n",
    "    else:\n",
    "        course_keywords[f\"{code}_{i}\"] = item\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"course_top_keywords.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(course_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Top keywords saved to {output_file}\")\n",
    "\n",
    "# Display sample of the data (first 3 courses)\n",
    "print(\"\\nSample of extracted keywords:\")\n",
    "sample_count = min(3, len(course_codes))\n",
    "for i, code in enumerate(list(course_keywords.keys())[:sample_count]):\n",
    "    print(f\"\\nCourse {code}:\")\n",
    "    for item in course_keywords[code][:5]:  # Show only top 5 for the sample\n",
    "        print(f\"  {item['term']}: {item['score']:.4f}\")\n",
    "    if len(course_keywords[code]) > 5:\n",
    "        print(\"  ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar courses to IB109:\n",
      "  PV197 - GPU Programming: 0.0360\n",
      "  PA039 - Supercomputer Architecture and Intensive Computations: 0.0357\n",
      "  IV100 - Parallel and distributed computations: 0.0310\n",
      "  IV003 - Algorithms and Data Structures II: 0.0285\n",
      "  C2143 - Design of algorithms in life sciences - seminary: 0.0275\n",
      "  MA015 - Graph Algorithms: 0.0266\n",
      "  IB002 - Algorithms and data structures I: 0.0254\n",
      "  C2142 - Design of algorithms in life sciences: 0.0251\n",
      "  IB114 - Introduction to Programming and Algorithms II: 0.0247\n",
      "  PV281 - Programming in Rust: 0.0244\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate similarity between courses based on shared keywords\n",
    "def calculate_course_similarity(course1_code, course2_code, course_keywords, weight_by_score=True):\n",
    "    if course1_code not in course_keywords or course2_code not in course_keywords:\n",
    "        return 0.0\n",
    "\n",
    "    keywords1 = {item[\"term\"]: item[\"score\"] for item in course_keywords[course1_code]}\n",
    "    keywords2 = {item[\"term\"]: item[\"score\"] for item in course_keywords[course2_code]}\n",
    "\n",
    "    shared_keywords = set(keywords1.keys()) & set(keywords2.keys())\n",
    "\n",
    "    if not shared_keywords:\n",
    "        return 0.0\n",
    "\n",
    "    if weight_by_score:\n",
    "        similarity = sum(keywords1[term] * keywords2[term] for term in shared_keywords)\n",
    "        total_possible = sum(keywords1.values()) * sum(keywords2.values())\n",
    "        if total_possible > 0:\n",
    "            similarity = similarity / total_possible\n",
    "    else:\n",
    "        similarity = len(shared_keywords) / len(set(keywords1.keys()) | set(keywords2.keys()))\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def find_most_similar_courses(course_code, course_keywords, top_n=5):\n",
    "    if course_code not in course_keywords:\n",
    "        return []\n",
    "\n",
    "    similarities = []\n",
    "    for other_code in course_keywords:\n",
    "        if other_code != course_code:\n",
    "            sim = calculate_course_similarity(course_code, other_code, course_keywords)\n",
    "            similarities.append((other_code, sim))\n",
    "\n",
    "    # Sort by similarity score in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:top_n]\n",
    "\n",
    "query = 'IB109'\n",
    "if query in course_keywords:  # Natural Language Processing in Practice\n",
    "    print(f\"Most similar courses to {query}:\")\n",
    "    similar_courses = find_most_similar_courses(query, course_keywords, top_n=10)\n",
    "    for code, sim in similar_courses:\n",
    "        course_name = \"\"\n",
    "        for idx, c_code in enumerate(course_codes):\n",
    "            if c_code == code:\n",
    "                course_name = courses[idx]['NAME']\n",
    "                break\n",
    "        print(f\"  {code} - {course_name}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21106"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(course_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building intersection matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating intersections: 100%|██████████| 21106/21106 [02:41<00:00, 130.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to sparse uint8 matrix...\n",
      "Saving sparse uint8 intersection matrix to ../web/backend/assets/intersection_matrix_tfidf_u8.npz...\n",
      "Saving course indices mapping to ../web/backend/assets/intersection_course_indices.pkl...\n",
      "Intersection matrix and indices saved.\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def create_intersection_matrix(course_keywords):\n",
    "    course_codes_list = list(course_keywords.keys())\n",
    "    course_indices = {code: idx for idx, code in enumerate(course_codes_list)}\n",
    "    n_courses = len(course_codes_list)\n",
    "\n",
    "    intersection_matrix = np.zeros((n_courses, n_courses), dtype=np.uint8)\n",
    "\n",
    "    print(\"Building intersection matrix...\")\n",
    "    keyword_sets = {}\n",
    "    for code, keywords in course_keywords.items():\n",
    "        keywords = [item[\"term\"] for item in keywords]\n",
    "        keyword_sets[code] = set(keywords)\n",
    "\n",
    "    # keyword_sets = {code: set(keywords) for code, keywords in course_keywords.items()}\n",
    "\n",
    "    for i, code1 in tqdm(enumerate(course_codes_list), total=n_courses, desc=\"Calculating intersections\"):\n",
    "        keywords1_set = keyword_sets[code1]\n",
    "        intersection_matrix[i, i] = len(keywords1_set)\n",
    "\n",
    "        for j in range(i + 1, n_courses):\n",
    "            code2 = course_codes_list[j]\n",
    "            keywords2_set = keyword_sets[code2]\n",
    "\n",
    "            intersection_count = len(keywords1_set.intersection(keywords2_set))\n",
    "\n",
    "            intersection_matrix[i, j] = intersection_count\n",
    "            intersection_matrix[j, i] = intersection_count\n",
    "\n",
    "    return intersection_matrix, course_indices\n",
    "\n",
    "intersection_matrix_u8, course_indices = create_intersection_matrix(course_keywords)\n",
    "print(\"Converting to sparse uint8 matrix...\")\n",
    "sparse_intersection_matrix_u8 = sparse.csr_matrix(intersection_matrix_u8)\n",
    "\n",
    "os.makedirs('assets', exist_ok=True)\n",
    "\n",
    "matrix_filename = '../web/backend/assets/intersects_tfidf.npz'\n",
    "print(f\"Saving sparse uint8 intersection matrix to {matrix_filename}...\")\n",
    "sparse.save_npz(matrix_filename, sparse_intersection_matrix_u8)\n",
    "\n",
    "indices_reverse = {idx: code for code, idx in course_indices.items()}\n",
    "indices_filename = '../web/backend/assets/intersection_course_indices.pkl'\n",
    "print(f\"Saving course indices mapping to {indices_filename}...\")\n",
    "with open(indices_filename, 'wb') as f:\n",
    "    pickle.dump(course_indices, f)\n",
    "\n",
    "\n",
    "print(\"Intersection matrix and indices saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving course indices mapping to ../web/backend/assets/course_indices_tfidf.pkl...\n"
     ]
    }
   ],
   "source": [
    "indices_reverse = {idx: code for code, idx in course_indices.items()}\n",
    "indices_filename = '../web/backend/assets/course_indices_tfidf.pkl'\n",
    "print(f\"Saving course indices mapping to {indices_filename}...\")\n",
    "with open(indices_filename, 'wb') as f:\n",
    "    pickle.dump(indices_reverse, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_indices = pickle.load(open(\"../web/backend/assets/course_indices_tfidf.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sparse similarity matrix...\n",
      "Top 15 most similar courses to ['IB111', 'IB109']:\n",
      "IB113  Introduction to Programming and Algorithms: Similarity score = 9.0000\n",
      "IB114  Introduction to Programming and Algorithms II: Similarity score = 9.0000\n",
      "IV003  Algorithms and Data Structures II: Similarity score = 9.0000\n",
      "IA101  Algorithmics for Hard Problems: Similarity score = 8.0000\n",
      "PA039  Supercomputer Architecture and Intensive Computations: Similarity score = 8.0000\n",
      "PV197  GPU Programming: Similarity score = 8.0000\n",
      "C2142  Design of algorithms in life sciences: Similarity score = 8.0000\n",
      "C2143  Design of algorithms in life sciences - seminary: Similarity score = 8.0000\n",
      "IB002  Algorithms and data structures I: Similarity score = 7.0000\n",
      "TI2011  Didactics - Electrical and Electronics: Similarity score = 7.0000\n",
      "ISKM72  Basics of Algorithmic Thinking: Similarity score = 7.0000\n",
      "IA012  Complexity: Similarity score = 6.0000\n",
      "IV104  Programming Seminar: Similarity score = 6.0000\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load the sparse similarity matrix\n",
    "print(\"Loading the sparse similarity matrix...\")\n",
    "sparse_similarity_matrix = sparse.load_npz('../web/backend/assets/intersects_tfidf.npz')\n",
    "\n",
    "# course_indices = pickle.load(open(indices_filename, 'rb'))\n",
    "\n",
    "similarity_matrix = sparse_similarity_matrix.toarray()\n",
    "\n",
    "def find_top_courses_multiple(idx_liked: List[int], idx_disliked: List[int], matrix: sp.csr_matrix, m: int) -> List[Tuple[int, float]]:\n",
    "    matrix = matrix.toarray().astype(np.float32)\n",
    "    liked_scores = matrix[idx_liked]\n",
    "    disliked_scores = matrix[idx_disliked]\n",
    "\n",
    "    summed = liked_scores.sum(axis=0) - disliked_scores.sum(axis=0)\n",
    "\n",
    "    course_scores = [(i, score) for i, score in enumerate(summed)]\n",
    "    course_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return course_scores[:m]\n",
    "\n",
    "liked_codes = ['IB111', 'IB109']\n",
    "disliked_codes = ['IB000']\n",
    "\n",
    "ctoi = {code: idx for idx, code in enumerate(course_codes)}\n",
    "\n",
    "liked_ids = [ctoi[code] for code in liked_codes]\n",
    "disliked_ids = [ctoi[code] for code in disliked_codes]\n",
    "\n",
    "n = 15\n",
    "\n",
    "top_course_ids = find_top_courses_multiple(liked_ids, disliked_ids, sparse_similarity_matrix, n)\n",
    "res = []\n",
    "for idx, score  in top_course_ids:\n",
    "    if idx in liked_ids or idx in disliked_ids:\n",
    "        continue\n",
    "    course = courses[idx]\n",
    "    res.append((course['CODE'], course['NAME'], score))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Top {n} most similar courses to {liked_codes}:\")\n",
    "for code, name, score in res:\n",
    "    print(f\"{code}  {name}: Similarity score = {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21106, 21106)\n",
      "(21106,)\n",
      "IB113\n",
      "IB114\n",
      "IV003\n",
      "IA101\n",
      "PA039\n",
      "PV197\n",
      "C2142\n",
      "C2143\n",
      "IB002\n",
      "TI2011\n",
      "ISKM72\n",
      "IA012\n",
      "IV104\n",
      "MA017\n",
      "PA093\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "loaded_sparse_matrix = sp.load_npz(\"../web/backend/assets/intersects_tfidf.npz\")\n",
    "kwd_intersects = loaded_sparse_matrix.toarray().astype(np.float32)\n",
    "course_indices = pickle.load(open(\"../web/backend/assets/course_indices_tfidf.pkl\", \"rb\"))\n",
    "\n",
    "def find_top_courses(\n",
    "    idx_liked: List[int], idx_disliked: List[int], matrix: sp.csr_matrix\n",
    ") -> List[Tuple[int, float]]:\n",
    "    liked_scores = matrix[idx_liked]\n",
    "    disliked_scores = matrix[idx_disliked]\n",
    "\n",
    "    summed = liked_scores.sum(axis=0) - disliked_scores.sum(axis=0)\n",
    "\n",
    "    course_scores = [(i, score) for i, score in enumerate(summed)]\n",
    "    course_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return course_scores\n",
    "\n",
    "\n",
    "def recommend_courses_keywords_tfidf(\n",
    "    liked_ids: List[str],\n",
    "    disliked_ids: List[str],\n",
    "    skipped_ids: List[str],\n",
    "    n: int,\n",
    "):\n",
    "    top_courses = find_top_courses(liked_ids, disliked_ids, kwd_intersects)\n",
    "\n",
    "    res = []\n",
    "    for idx, _ in top_courses:\n",
    "        if idx in liked_ids or idx in disliked_ids or idx in skipped_ids:\n",
    "            continue\n",
    "        # course = courseClient.get_course_by_id(idx)\n",
    "        course_code = course_indices[idx]\n",
    "        course = course_code\n",
    "        if course is not None:\n",
    "            res.append(course)\n",
    "        if len(res) == n:\n",
    "            break\n",
    "\n",
    "    return res\n",
    "\n",
    "liked_codes = ['IB111', 'IB109']\n",
    "disliked_codes = ['IB000']\n",
    "\n",
    "ctoi = {code: idx for idx, code in enumerate(course_codes)}\n",
    "\n",
    "liked_ids = [ctoi[code] for code in liked_codes]\n",
    "disliked_ids = [ctoi[code] for code in disliked_codes]\n",
    "skipped_ids = []\n",
    "\n",
    "rec_codes = recommend_courses_keywords_tfidf(liked_ids, disliked_ids, skipped_ids, n)\n",
    "\n",
    "for code in rec_codes:\n",
    "    print(code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmi-starterpack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
